---
title: "大模型面试题-基础篇"
description: "大模型面试题-基础篇"
keywords: ["LLM", "面试"]
author: "Arkin"
date: 2025-08-04T17:45:10+08:00
lastmod: 2025-09-19T17:45:10+08:00
draft: false
tags: ["", "什么是大语言模型？"]
categories: ["面试题", "大模型"]
aliases: []
image: "img/featured-image.jpg"
toc: true
readingTime: true
showWordCount: true
showDateUpdated: true
---

> 基础不牢，地动山摇。很多人在忙着开发大模型，却对最核心的概念理解不全。基础题，正是行业内经过验证的共识，是必须掌握的底层逻辑。吃透它，你才能在复杂应用中立于不败之地。

## Q&A

- ### 1. 简单介绍一下大语言模型（LLMs）？

  大模型：一般指**1亿**以上参数的模型，但是这个标准一直在升级，目前万亿参数以上的模型（eg: MinMax abab6.5）也有了。大语言模型（Large Language Models, LLMs）是针对语言的大模型。

  大语言模型的关键特点包括：

  - **大规模参数**：LLMs拥有大量参数（如GPT-3的175B或PaLM的540B），参数数量越多，模型对语言的捕捉能力越强，可以更好地理解上下文和生成流畅的语言。
  - **多任务处理能力**：经过训练后，LLMs 具备在多种语言任务上表现良好的能力，例如文本摘要、情感分析、机器翻译等。这是因为它们学会了在大数据集上归纳出各种语言模式和规律。
  - **上下文理解**：LLMs 可以根据上下文生成有逻辑和连贯的回应，这使得它们特别适合对话和内容创作任务。模型可以“记住”一段对话中的重要细节，并在后续对话中保持一致性。
  - **自监督学习**：大语言模型主要依赖自监督学习，使用未标注的数据进行训练，通过预测下一个词、填补空白或匹配句子等方式来学习语言结构。
  - **通用型和扩展性**：LLMs 可以迁移到多种任务和领域，经过少量微调就能在专门的任务（如医学文本分析或法律文件摘要）中发挥作用。

  尽管大语言模型在许多任务上表现出色，但它们也有一些局限性，比如生成错误信息、偏见问题以及需要高昂的计算资源。

  *参考资料：不同尺寸大模型在中文的能力评测，目前已囊括115个大模型，覆盖 chatgpt、gpt4o、百度文心一言、阿里通义千问、讯飞星火、商汤SenseCha、minimax等商用大模型，以及百川、qwen2、glm4、yi、书生internLM2、llama3等开源大模型，多维度能力评估。*

  *参考链接：https://github.com/jeinlee1991/chinese-llm-benchmark*

  ![中文语言大模型综合能力基准测试排行版0921](https://github.com/jeinlee1991/chinese-llm-benchmark/blob/main/pic/%E6%80%BB%E5%88%86.png?raw=true)

- ### 2. 大语言模型（LLMs）后面跟的175B、60B、540B等 指的是什么？

  在大语言模型名称后面跟随的数字如“175B”、“60B”或者“540B”指的是模型的参数量（Parameters），通常以“B”表示“billion”，即为“十亿”。这些参数数量直接影响模型的复杂性和计算能力：

  - **175B**：指模型参数为1750亿个参数，比如OpenAI的GPT-3。
  - **60B**：指的是模型有600亿个参数，比如Meta的LLM模型版本之一。
  - **540B**：指的是模型有5400亿个参数，比如Google的Pathways Language Model（PaLM）的较大版本。

  参数数量越多，模型能学习和捕捉的语言模式和语义信息也越丰富，但对计算资源的要求也随之增加。更大的参数规模通常可以提高模型的性能，特别是在处理复杂任务或生成高质量文本时。不过，增加参数并不总是直接等于更好的效果，模型的优化】数据质量和训练方法也同样重要。

- ### 3. 大语言模型（LLMs）具有什么优点？什么缺点？

  - **优点**：
    - **语言理解和生成能力强**：大语言模型可以基于大量文本数据进行训练，从而对自然语言的理解和生成有出色的表现。例如，可以生成连贯的文章、回答问题、甚至进行对话模拟，非常适用于闲聊式对话、写作辅助等场景。
    - **跨领域知识广泛**：大语言模型可以在医学、法律、工程等多个领域提供知识支持。这得益于他们在海量数据上训练，有一定的跨领域知识积累，用户可以咨询各种专业或非专业的问题。
    - **支持多语言**：很多大语言模型具备多语言能力，能够处理并理解多种语言，这在跨国公司或者多语言客户服务中尤其实用。例如，GPT-3和GPT-4支持英文、中文、法语等多种语言的理解和生成。
    - **快速部署和适应性**：由于大语言模型能够基于预训练好的模型进行微调，从而适应不同任务需求。例如，可以在模型的基础上微调特定领域数据，使其适用于具体领域，如法律助理、医疗咨询等。
  - **缺点**：
    - **缺乏事实准确性**：大语言模型在回答问题时，可能会产生看似合理但不准确的消息。这是因为模型本质上是基于模式匹配和概率分布进行生成，并非真正理解事实，因此在知识更新方面也容易滞后。
    - **计算资源消耗大**：大语言模型通常需要大量计算资源来训练和运行。例如，GPT-3模型的训练耗费了大量电力和硬件资源，在实际应用中成本也非常高，尤其是在实时响应场景中，模型推理速度也受到硬件限制。
    - **可能产生偏见**：大语言模型是基于大量互联网数据进行训练的，可能会反映出数据中的偏见，如性别、种族、地域等方面的问题。这些偏见会在模型输出中显现，可能会影响模型的公正性和适用性。
    - **隐私和安全问题**：模型在训练过程中，可能会接触到一些敏感或个人信息。如果这些信息在模型生成内容时不慎暴露，可能会带来隐私泄漏的风险，尤其在处理敏感数据的应用场景下。

- ### 4. 常见的大语言模型（LLMs）分类有哪些？

  大模型的分类可以有很多种，根据内容可以自洽就可以。

  - **根据输入内容分类**：
    - **语言大模型（NLP）**：
      - 指在自然语言处理（Natural Language Processing， NLP）领域中的一类大模型，通常用于处理文本数据和理解自然语言。这类大模型的主要特点是他们在大规模语料库上进行了训练，以学习自然语言的各种语法、语义和语境规则。
      - 例如：GPT 系列（OpenAI）、Bard（Google）、文心一言（百度）、Qwen（阿里）。
    - **视觉大模型（CV）**：
      - 指在计算机视觉（Computer Vision，CV）领域中使用的大模型，通常用于图像处理和分析。这类模型通过在大规模图像数据上进行训练，可以实现各种视觉任务，如图像分类、目标检测、图像分割、姿态估计、人脸识别等。
      - 例如：VIT系列（Google）、文心UFO、华为盘古CV、INTERN（商汤-书生）。
    - **多模态大模型**：
      - 指能够处理多种不同类型数据的大模型，如文本、图像、音频等多模态数据。这类模型结合了 NLP 和 CV 的能力，以实现对多模态信息的综合理解和分析，从而能够更全面地理解和处理复杂的数据。
      - 例如：DALL-E（OpenAI）、midjourney。
  - **按预训练任务分类**：
    - **自回归语言模型（Autoregressive Language Model）**：
      - 如GPT系列模型，仅通过前文预测下一词，适合生成任务。
      - **例子**：
        - **输入**：`"The cat sat on the"`
        - **输出（目标）**：`"mat"`
    - **自编码语言模型（Autoencoding Language Model）**：
      - 如BERT、通过掩码（masked language modeling）预测被遮挡的词，适合理解和分类任务。
      - **例子**：
        - **输入（带掩码）**：`"The cat [MASK] on the mat."`
        - **输出（目标）**：`"sat"`
    - **序列到序列语言模型（Seq2Seq Language Model）**：
      - 如T5、BART等，既可以生成文本也可以完成理解任务，在机器翻译、文本生成场景中有广泛应用。
      - **例子**：
        - **输入（被破坏的句子）**：`"The [MASK] sat [MASK] the mat."`
        - **输出（目标，完整句子）**：`"The cat sat on the mat."`
  - **按模型规模分类**：
    - **小规模模型**：
      - 如BERT Base、GPT-2 Small等，参数量通常在数千万到几亿之间，适合资源有限的设备或边缘计算。
    - **中等规模模型**：
      - 如BERT Large、GPT-2 Medium，参数量通常在几亿到几十亿之间，平衡性能和计算开销。
    - **大规模模型**：
      - 如GPT-3、PaLM、LLaMA等，参数可达数百亿至上万亿，性能优异但资源需求极高，适合需要复杂推理的任务。

- ### 5. 目前主流的LLMs开源模型体系有哪些？（Prefix Decoder，Causal Decoder 和 Encoder-Decoder 区别是什么？）

  - 在**预训练语言模型**时代，自然语言处理领域广泛采用了**预训练+微调**的范式，并诞生了以BERT为代表的编码器（Encoder-only）架构、以GPT为代表的解码器（Decoder-only）架构和以 T5 为代表的编码器-解码器（Encoder-decoder）架构的大规模预训练语言模型。
  - 随着 GPT 系列模型的成功发展，当前自然语言处理领域走向了生成式大语言模型的道路，解码器架构已经成为了目前大语言模型的主流架构。进一步，解码器架构还可以细分为两个变种架构，包括因果编码起（Causal Decoder）架构和前缀编码器（Prefix Decoder）架构。值得注意的是，学术界所提到的编码器架构时，通常指的都是因果编码器架构。

  下图针对三种架构（Causal Decoder，Prefix Decoder 和 Encoder-Decoder）进行了对比：

  



## 总结

在这里写文章的总结部分。



## 相关链接

- [原文视频](https://www.bilibili.com/video/BV1E4bczRES9?spm_id_from=333.788.videopod.episodes&vd_source=baf08b4f56da32601c712e9657f34742&p=2)
- [相关文章2](/posts/related-post-2)

---

*最后更新时间：2025-09-19T17:45:10+08:00*
