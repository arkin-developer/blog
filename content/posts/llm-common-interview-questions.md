---
title: "大模型面试题-基础篇（1）"
description: "大模型面试题-基础篇（1）"
keywords: ["LLM", "面试"]
author: "Arkin"
date: 2025-08-04T17:45:10+08:00
lastmod: 2025-08-04T17:45:10+08:00
draft: false
tags: ["LLM", "面试", "大模型", "人工智能"]
categories: ["人工智能"]
aliases: []
image: "img/featured-image.jpg"
toc: true
readingTime: true
showWordCount: true
showDateUpdated: true
---

> 基础不牢，地动山摇。很多人在忙着开发大模型，却对最核心的概念理解不全。基础题，正是行业内经过验证的共识，是必须掌握的底层逻辑。吃透它，你才能在复杂应用中立于不败之地。

## Q&A

- ### 1. 简单介绍一下大语言模型（LLMs）？

  大模型：一般指**1亿**以上参数的模型，但是这个标准一直在升级，目前万亿参数以上的模型（eg: MinMax abab6.5）也有了。大语言模型（Large Language Models, LLMs）是针对语言的大模型。

  大语言模型的关键特点包括：

  - **大规模参数**：LLMs拥有大量参数（如GPT-3的175B或PaLM的540B），参数数量越多，模型对语言的捕捉能力越强，可以更好地理解上下文和生成流畅的语言。
  - **多任务处理能力**：经过训练后，LLMs 具备在多种语言任务上表现良好的能力，例如文本摘要、情感分析、机器翻译等。这是因为它们学会了在大数据集上归纳出各种语言模式和规律。
  - **上下文理解**：LLMs 可以根据上下文生成有逻辑和连贯的回应，这使得它们特别适合对话和内容创作任务。模型可以“记住”一段对话中的重要细节，并在后续对话中保持一致性。
  - **自监督学习**：大语言模型主要依赖自监督学习，使用未标注的数据进行训练，通过预测下一个词、填补空白或匹配句子等方式来学习语言结构。
  - **通用型和扩展性**：LLMs 可以迁移到多种任务和领域，经过少量微调就能在专门的任务（如医学文本分析或法律文件摘要）中发挥作用。

  尽管大语言模型在许多任务上表现出色，但它们也有一些局限性，比如生成错误信息、偏见问题以及需要高昂的计算资源。

  *参考资料：不同尺寸大模型在中文的能力评测，目前已囊括115个大模型，覆盖 chatgpt、gpt4o、百度文心一言、阿里通义千问、讯飞星火、商汤SenseCha、minimax等商用大模型，以及百川、qwen2、glm4、yi、书生internLM2、llama3等开源大模型，多维度能力评估。*

  *参考链接：https://github.com/jeinlee1991/chinese-llm-benchmark*

  ![中文语言大模型综合能力基准测试排行版0921](https://github.com/jeinlee1991/chinese-llm-benchmark/blob/main/pic/%E6%80%BB%E5%88%86.png?raw=true)

- ### 2. 大语言模型（LLMs）后面跟的175B、60B、540B等 指的是什么？

  在大语言模型名称后面跟随的数字如“175B”、“60B”或者“540B”指的是模型的参数量（Parameters），通常以“B”表示“billion”，即为“十亿”。这些参数数量直接影响模型的复杂性和计算能力：

  - **175B**：指模型参数为1750亿个参数，比如OpenAI的GPT-3。
  - **60B**：指的是模型有600亿个参数，比如Meta的LLM模型版本之一。
  - **540B**：指的是模型有5400亿个参数，比如Google的Pathways Language Model（PaLM）的较大版本。

  参数数量越多，模型能学习和捕捉的语言模式和语义信息也越丰富，但对计算资源的要求也随之增加。更大的参数规模通常可以提高模型的性能，特别是在处理复杂任务或生成高质量文本时。不过，增加参数并不总是直接等于更好的效果，模型的优化】数据质量和训练方法也同样重要。

- ### 3. 大语言模型（LLMs）具有什么优点？什么缺点？

  - **优点**：
    - **语言理解和生成能力强**：大语言模型可以基于大量文本数据进行训练，从而对自然语言的理解和生成有出色的表现。例如，可以生成连贯的文章、回答问题、甚至进行对话模拟，非常适用于闲聊式对话、写作辅助等场景。
    - **跨领域知识广泛**：大语言模型可以在医学、法律、工程等多个领域提供知识支持。这得益于他们在海量数据上训练，有一定的跨领域知识积累，用户可以咨询各种专业或非专业的问题。
    - **支持多语言**：很多大语言模型具备多语言能力，能够处理并理解多种语言，这在跨国公司或者多语言客户服务中尤其实用。例如，GPT-3和GPT-4支持英文、中文、法语等多种语言的理解和生成。
    - **快速部署和适应性**：由于大语言模型能够基于预训练好的模型进行微调，从而适应不同任务需求。例如，可以在模型的基础上微调特定领域数据，使其适用于具体领域，如法律助理、医疗咨询等。
  - **缺点**：
    - **缺乏事实准确性**：大语言模型在回答问题时，可能会产生看似合理但不准确的消息。这是因为模型本质上是基于模式匹配和概率分布进行生成，并非真正理解事实，因此在知识更新方面也容易滞后。
    - **计算资源消耗大**：大语言模型通常需要大量计算资源来训练和运行。例如，GPT-3模型的训练耗费了大量电力和硬件资源，在实际应用中成本也非常高，尤其是在实时响应场景中，模型推理速度也受到硬件限制。
    - **可能产生偏见**：大语言模型是基于大量互联网数据进行训练的，可能会反映出数据中的偏见，如性别、种族、地域等方面的问题。这些偏见会在模型输出中显现，可能会影响模型的公正性和适用性。
    - **隐私和安全问题**：模型在训练过程中，可能会接触到一些敏感或个人信息。如果这些信息在模型生成内容时不慎暴露，可能会带来隐私泄漏的风险，尤其在处理敏感数据的应用场景下。

- ### 4. 常见的大语言模型（LLMs）分类有哪些？

  大模型的分类可以有很多种，根据内容可以自洽就可以。

  - **根据输入内容分类**：
    - **语言大模型（NLP）**：
      - 指在自然语言处理（Natural Language Processing， NLP）领域中的一类大模型，通常用于处理文本数据和理解自然语言。这类大模型的主要特点是他们在大规模语料库上进行了训练，以学习自然语言的各种语法、语义和语境规则。
      - 例如：GPT 系列（OpenAI）、Bard（Google）、文心一言（百度）、Qwen（阿里）。
    - **视觉大模型（CV）**：
      - 指在计算机视觉（Computer Vision，CV）领域中使用的大模型，通常用于图像处理和分析。这类模型通过在大规模图像数据上进行训练，可以实现各种视觉任务，如图像分类、目标检测、图像分割、姿态估计、人脸识别等。
      - 例如：VIT系列（Google）、文心UFO、华为盘古CV、INTERN（商汤-书生）。
    - **多模态大模型**：
      - 指能够处理多种不同类型数据的大模型，如文本、图像、音频等多模态数据。这类模型结合了 NLP 和 CV 的能力，以实现对多模态信息的综合理解和分析，从而能够更全面地理解和处理复杂的数据。
      - 例如：DALL-E（OpenAI）、midjourney。
  - **按预训练任务分类**：
    - **自回归语言模型（Autoregressive Language Model）**：
      - 如GPT系列模型，仅通过前文预测下一词，适合生成任务。
      - **例子**：
        - **输入**：`"The cat sat on the"`
        - **输出（目标）**：`"mat"`
    - **自编码语言模型（Autoencoding Language Model）**：
      - 如BERT、通过掩码（masked language modeling）预测被遮挡的词，适合理解和分类任务。
      - **例子**：
        - **输入（带掩码）**：`"The cat [MASK] on the mat."`
        - **输出（目标）**：`"sat"`
    - **序列到序列语言模型（Seq2Seq Language Model）**：
      - 如T5、BART等，既可以生成文本也可以完成理解任务，在机器翻译、文本生成场景中有广泛应用。
      - **例子**：
        - **输入（被破坏的句子）**：`"The [MASK] sat [MASK] the mat."`
        - **输出（目标，完整句子）**：`"The cat sat on the mat."`
  - **按模型规模分类**：
    - **小规模模型**：
      - 如BERT Base、GPT-2 Small等，参数量通常在数千万到几亿之间，适合资源有限的设备或边缘计算。
    - **中等规模模型**：
      - 如BERT Large、GPT-2 Medium，参数量通常在几亿到几十亿之间，平衡性能和计算开销。
    - **大规模模型**：
      - 如GPT-3、PaLM、LLaMA等，参数可达数百亿至上万亿，性能优异但资源需求极高，适合需要复杂推理的任务。

- ### 5. 目前主流的LLMs开源模型体系有哪些？（Prefix Decoder，Causal Decoder 和 Encoder-Decoder 区别是什么？）

  在**预训练语言模型**时代，自然语言处理领域广泛采用了**预训练+微调**的范式，并诞生了以BERT为代表的编码器（Encoder-only）架构、以**GPT为代表的解码器（Decoder-only）架构和以 T5 为代表的编码器-解码器（Encoder-decoder）架构**的大规模预训练语言模型。
  
  随着 **GPT 系列模型的成功发展**，当前自然语言处理领域走向了生成式大语言模型的道路，解码器架构已经成为了目前大语言模型的主流架构。进一步，解码器架构还可以细分为两个变种架构，包括**因果编码起（Causal Decoder）**架构和**前缀编码器**（Prefix Decoder）架构。值得注意的是，学术界所提到的编码器架构时，通常指的都是因果编码器架构。
  
  下图针对三种架构（Causal Decoder，Prefix Decoder 和 Encoder-Decoder）进行了对比：
  
  ![image-20250922122538906](https://mr-lai.oss-cn-zhangjiakou.aliyuncs.com/macminim4image-20250922122538906.png)
  
  ##### **Encoder-Decoder**
  
  - Encoder-Decoder 架构是自然语言处理领域里的一种经典的模型结构，广泛应用于如机器翻译等多项任务。原始的 Transformer 模型也是用了这一架构，组合了两个分别担任编码器和解码器的 Transfomer 模块。
  - 如下图所示，此架构在编码器端采用了双向自注意力机制对输入信息进行编码处理，而在解码器端则使用了交叉注意力与掩码自注意力机制，进而通过自回归的方式对输出进行生成
  - 基于编码器-解码器设计的预训练语言模型在众多自然语言理解与生成任务中展现出了优异的性能，但是目前只有如 FLAN-TS 等少数大语言模型是基于编码器-解码器架构构建而成的。
  
  ![image-20250922123112350](https://mr-lai.oss-cn-zhangjiakou.aliyuncs.com/macminim4image-20250922123112350.png)
  
  **Causal Decoder**
  
  - Causal LM 是因果语言模型，目前流行的大多数模型都是这种结构，别无他因，因为GPT系列模型内部结构就是它，还有开源界的 LLaMA 也是。**Causal Decoder 架构的经典代表就是 GPT 系列模型，使用的是单向注意力掩码，以确保每个输入 token 只能注意到过去的 token 和它本身，输入和输出的 token 通过 Decoder 以相同的方式进行处理。**
  - 在下图中，灰色代表对应的两个 token 互相之间看不到，否则就代表可以看到。例如，“Survery” 可以看到前面的 “A”，但是看不到后面的 “of”。Causal Decoder 的 sequence mask 矩阵是一个经典的下三角形矩阵。
  - 在因果解码器架构中，最具有代表性的模型就是 OpenAI 推出的 GPT 系列。伴随着 GPT-3 的成功，因果解码器被广泛采用于各种大语言模型中，包括 BLOOM、LLaMA（Meta）等。
  
  ![image-20250922124434537](https://mr-lai.oss-cn-zhangjiakou.aliyuncs.com/macminim4image-20250922124434537.png)
  
  ##### **Prefix Decoder**
  
  - Prefix Decoder 架构也被称为非因果解码器架构，对于因果解码器的掩码机制进行了修改。该架构和因果解码器一样，仅仅使用了解码器组件。
  - 与之不同的是，该架构参考了编码器-解码器的设计，对于输入和输出部分进行了特定处理。如下图所示，前缀解码器对于输入（前缀）部份使用双向注意力进行编码，而对于输出部分利用单向的掩码注意力利用该词元本身和前面的词元进行自回归地预测。
  - 与编码器-解码器不同的是，前缀解码器在编码和解码的过程中是共享参数的，并没有划分为独立的解码器和编码器。
  - 当前，基于前缀解码器架构的代表性大语言模型包括 GLM-130B 和 U-PaLM（Google）。
  
  ![image-20250922125902376](https://mr-lai.oss-cn-zhangjiakou.aliyuncs.com/macminim4image-20250922125902376.png)
  
  ##### **总结**
  
  Prefix Decoder，Causal Decoder 和 Encoder-Decoder 区别在于 attention mask 不同
  
  - Encoder-Decoder（代表：T5）：
    - 在输入上采用双向注意力机制，对问题的编码理解更充分。
    - 使用任务：在偏理解的 NLP 任务上效果更好。
    - 在长文本生成任务上效果差，训练效率低。
  - Causal Decoder（代表：GPT系列）：
    - 自回归语言模型，预训练和下游应用是完全一致的，严格遵循只有后面的 token 才能看到前面的 token 的规则。
    - 适用任务：文本生成任务效果好。
    - 训练效率高，zero-shot 能力更强，具有涌现能力。
  - Prefix Decoder（代表：GLM）：
    - 特点：prefix 部分的 token 能够互相看到。
    - 使用任务：文本生成任务效果好。



## 相关链接

- [B站原文视频](https://www.bilibili.com/video/BV1E4bczRES9?spm_id_from=333.788.videopod.episodes&vd_source=baf08b4f56da32601c712e9657f34742&p=2)

---

*感谢阅读，欢迎交流与反馈。*

*我的邮箱📮 arkin-dev@qq.com（需要交流请发邮件）。*
