---
title: "大模型面试题-基础篇（2）"
description: "大模型面试题-基础篇（2）"
keywords: ["LLM", "面试"]
author: "Arkin"
date: 2025-08-07T17:45:10+08:00
lastmod: 2025-09-19T17:45:10+08:00
draft: false
tags: ["LLM", "面试", "大模型", "人工智能"]
categories: ["人工智能"]
aliases: []
image: "img/featured-image.jpg"
toc: true
readingTime: true
showWordCount: true
showDateUpdated: true
---

> 基础不牢，地动山摇。很多人在忙着开发大模型，却对最核心的概念理解不全。基础题，正是行业内经过验证的共识，是必须掌握的底层逻辑。吃透它，你才能在复杂应用中立于不败之地。

{{< katex >}}

## Q&A

- ### 1. LLMs 中，常用的预训练任务包括哪些？（LLMs 的训练目标是什么？）

  在进行模型的大规模预训练时，往往需要设计合适的自监督预训练任务，使得模型能够从海量无标注数据中学习到广泛的语义知识与世界知识。

  目前，常用的预训练任务主要分为三类，包括 **语言建模（Language Modeling，LM）**、**去除自编码（Denoising Autoencoding，DAE）**以及**混合去噪器（Mixture-of-Denoisers,，MoD）**。

  下图展示了语言建模和去噪自编码个字的输入与输出示例。

  ![image-20250923120242372](https://mr-lai.oss-cn-zhangjiakou.aliyuncs.com/macminim4image-20250923120242372.png)

  - **语言建模（Language Modeling，LM）**

    语言建模任务是目前绝大部分大语言模型广泛采用的预训练任务。该任务的核心在于“预测下一个词元”，并且经常被应用于训练基于解码器的大语言模型，例如 GPT-3 和 PaLM 等。

    形式化来说，给定一个词元序列 \\(u = u_1, …, u_T\\)，语言建模任务的目标定义为词元的预测任务：基于序列中当前位置之前的词元序列 \\(u_{<t}\\)，采用自回归的方式对于目标词元 \\(u_t\\) 进行预测。在训练过程中，模型通常根据以下的似然函数进行优化：
    $$
    L_{LM}(u) = \sum_{t=1}^{T} \log P(u_t \mid u_{<t})
    $$
    具体含义如下：

    - \\(u\\) 表示一个包含了 \\(T\\) 个单词或字的句子，具体来说就是一个序列。
    - \\(L_{LM}(u)\\) 表示语言建模对该句子的损失函数。
    - \\(P(u_t \mid u_{<t})\\) 表示在已知前 \\(t-1\\) 个词元的条件下，第 \\(t\\) 个词元 \\(u_t\\) 出现的概率。
    - \\(\\log P(u_t \mid u_{<t})\\) 表示该条件概率的对数。
    - 整个公式求和表示在序列中从 \\(t=1\\) 到 \\(T\\) 所有词的对数概率的总和。

    此外，从本质上看，基于语言建模的预训还可以看作是一种多任务学习过程。例如：

    - 在预测句子前缀“这部电影剧情饱满，演员表演得也很棒，非常好看”中的`好看`时，模型实际上在进行情感分析任务的语义学习。
    - 在预测句子前缀“小明有三块糖，给了小红两块糖，因此还剩下一块糖”中的`一块糖`时，则是在进行数学算术任务的语义学习。

    可以列举出来更多类似的例子，覆盖更广的任务范围。因此，基于大规模文本预料的预训练任务能够潜在地学习到解决众多任务的相关知识与能力。

    训练效率：**Prefix Decoder < Causal Decoder**

    - **Causal Decoder** 结构会在所有 token 上计算损失，而 **Prefix Decoder** 只会在输出上计算损失。
  
  - **去噪自编码（Denosising Autoencoding，DAE）**
  
    - 除了传统的语言建模任务外，去噪自编码任务是另一种常见的语言模型预训练任务，广泛用于 BERT、T5 等预训练语言模型中。
    - 在去噪自编码任务中，输入文本经过一系列随机替换或删除操作，形成损坏的文本 \\(\\tilde{u}\\)。模型的目标是根据这些损坏的文本恢复出被替换或删除的词元片段 \\(u\\)。去噪自编码的训练目标可以用以下数学公式表示：
  
    $$
    L_{DAE}(u) = \log P(u \mid \tilde{u})
    $$
  
    - **与语言建模相比，去噪自编码任务的实现更为复杂**，需要设定额外的优化策略，如词元替换策略、替换片段长度、替换词元比例等。这些策略的选择会直接影响模型的训练效果。尽管去噪自编码任务在许多预训练语言模型中得到了广泛应用，然而，相比于语言建模任务，目前完全使用去噪自编码进行预训练的纯语言模型却还较为有限。代表性的模型包括 FLAN-T5。
  
  - **混合去噪器（Mixture-of-Denoisers, MoD）**
  
    - 混合去噪器，通过将语言建模和去噪自编码的目标划分为不同类型的去噪任务，对于预训练任务进行了统一建模。
  
- ### 2. LLMs 中，涌现能力是什么？

  - 什么是“涌现能力”？
  - 等一个复杂系统由很多微小的个体构成，这些微小个体凑到一起，互相作用，当数量足够多时，在宏观层面上展现出微观个体无法解释的特殊现象，就可以称之为“涌现现象”。
    - 


## 总结

在这里写文章的总结部分。

## 相关链接

- [相关文章1](/posts/related-post-1)
- [相关文章2](/posts/related-post-2)

---

*感谢阅读，欢迎交流与反馈。*

*我的邮箱📮 arkin-dev@qq.com（需要交流请发邮件）。*
