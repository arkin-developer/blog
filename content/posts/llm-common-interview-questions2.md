---
title: "大模型面试题-基础篇（2）"
description: "大模型面试题-基础篇（2）"
keywords: ["LLM", "面试"]
author: "Arkin"
date: 2025-08-07T17:45:10+08:00
lastmod: 2025-09-19T17:45:10+08:00
draft: false
tags: ["LLM", "面试", "大模型", "人工智能"]
categories: ["人工智能"]
aliases: []
image: "img/featured-image.jpg"
toc: true
readingTime: true
showWordCount: true
showDateUpdated: true
---

> 基础不牢，地动山摇。很多人在忙着开发大模型，却对最核心的概念理解不全。基础题，正是行业内经过验证的共识，是必须掌握的底层逻辑。吃透它，你才能在复杂应用中立于不败之地。

## Q&A

- ### 1. LLMs 中，常用的预训练任务包括哪些？（LLMs 的训练目标是什么？）

  在进行模型的大规模预训练时，往往需要设计合适的自监督预训练任务，使得模型能够从海量无标注数据中学习到广泛的语义知识与世界知识。

  目前，常用的预训练任务主要分为三类，包括 **语言建模（Language Modeling，LM）**、**去除自编码（Denoising Autoencoding，DAE）**以及**混合去噪器（Mixture-of-Denoisers,，MoD）**。

  下图展示了语言建模和去噪自编码个字的输入与输出示例。

  ![image-20250923120242372](https://mr-lai.oss-cn-zhangjiakou.aliyuncs.com/macminim4image-20250923120242372.png)

  - **语言建模（Language Modeling，LM）**

    语言建模任务是目前绝大部分大语言模型广泛采用的预训练任务。该任务的核心在于“预测下一个词元”，并且经常被应用于训练基于解码器的大语言模型，例如 GPT-3 和 PaLM 等。

    形式化来说，给定一个词元序列 　在这里写第一部分的内容。$u = u_1, …, u_T$，语言建模任务的目标定义为词元的预测任务：基于序列中当前位置之前的词元序列 $u_{<t}$，采用自回归的方式对于目标词元 $u_t$ 进行预测。在训练过程中，模型通常根据以下的似然函数进行优化：
    $$
    L_{LM}(u) = \sum_{t=1}^{T} \log P(u_t \mid u_{<t})
    $$
    具体含义如下：

    - $u$ 表示一个包含了 $T$ 个单词或字的句子，具体来说就是一个序列。
    - $L_{LM}(u)$ 表示语言建模对该句子的损失函数。
    - $P(u_t \mid u_{<t})$ 表示在已知前 $t-1$ 个词元的条件下，第 $t$ 个词元 $u_t$ 出现的概率。
    - $\log P(u_t \mid u_{<t})$ 表示该条件概率的对数。
    - 整个公式求和表示在序列中从 $t=1$ 到 $T$ 所有词的对数概率的总和。

    此外，从本质上看，基于语言建模的预训还可以看作是一种多任务学习过程。例如：

    - 在预测句子前缀“这部电影剧情饱满，演员表演得也很棒，非常好看”中的`好看`时，模型实际上在进行情感分析任务的语义学习。
    - 在预测句子前缀“小明有三块糖，给了小红两块糖，因此还剩下一块糖”中的`一块糖`时，则是在进行数学算术任务的语义学习。

    可以列举出来更多类似的例子，覆盖更广的任务范围。因此，基于大规模文本预料的预训练任务能够潜在地学习到解决众多任务的相关知识与能力。

    训练效率：Prefix Decoder < Causal Decoder

    - 

### 第二部分

在这里写第二部分的内容。

## 总结

在这里写文章的总结部分。

## 相关链接

- [相关文章1](/posts/related-post-1)
- [相关文章2](/posts/related-post-2)

---

*感谢阅读，欢迎交流与反馈。*

*我的邮箱📮 arkin-dev@qq.com（需要交流请发邮件）。*
